#!/usr/bin/env python3
"""
kafka_streaming/inference_consumer.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ æ‰¹é‡æ¶ˆè´¹ Kafka â†’ æ¨ç†
â€¢ baseline_model + current_model äº§ç”Ÿä¸¤ç»„é¢„æµ‹
â€¢ è®°å½•å»¶è¿Ÿ / CPU / GPU / Cold-start / container_latency
â€¢ RTT_ms, throughput_s
â€¢ æœ€åæŠŠæ‰€æœ‰æ‰¹æ¬¡çš„ perf å†™å…¥ CSV å¹¶ä¸Šä¼ åˆ° MinIO
"""
import os
import sys
import io
import json
import time
import queue
import threading
import hashlib
from datetime import datetime
import pandas as pd
import numpy as np
import joblib
import torch
import torch.nn as nn
import psutil
from kafka import KafkaConsumer

from shared.metric_logger import log_metric
from shared.minio_helper  import s3, save_np, save_bytes, BUCKET
from shared.config        import (
    KAFKA_TOPIC, KAFKA_SERVERS, BATCH_SIZE, CONSUME_IDLE_S,
    MODEL_DIR, RESULT_DIR, AUTO_OFFSET_RESET, ENABLE_AUTO_COMMIT
)
from shared.features      import FEATURE_COLS
from shared.profiler      import Timer

# -------- å…¼å®¹æ—§ Baseline --------
ROOT = os.path.join(os.path.dirname(__file__), "..")
sys.path.insert(0, ROOT)
try:
    from ml.train_offline_full import MLPBaseline
except ModuleNotFoundError:
    from ml.train_offline import MLPBaseline

device = "cuda" if torch.cuda.is_available() else "cpu"
proc   = psutil.Process()

# -------- ç›®å½• & helper --------
os.makedirs(f"/mnt/pvc/{RESULT_DIR}/timing", exist_ok=True)
os.makedirs("/mnt/pvc/models",               exist_ok=True)

def _fetch(key: str) -> bytes:
    return s3.get_object(Bucket=BUCKET, Key=f"{MODEL_DIR}/{key}")["Body"].read()

def _bytes_to_model(raw: bytes) -> nn.Module:
    obj = torch.load(io.BytesIO(raw), map_location=device)
    if isinstance(obj, nn.Module):
        return obj.to(device).eval()
    mdl = MLPBaseline(len(FEATURE_COLS)).to(device)
    mdl.load_state_dict(obj)
    mdl.eval()
    return mdl

# -------- è½½å…¥ scaler --------
with open("/mnt/pvc/models/scaler.pkl", "wb") as f:
    f.write(_fetch("scaler.pkl"))
scaler = joblib.load("/mnt/pvc/models/scaler.pkl")

# -------- baseline_model (æ°¸ä¸çƒ­é‡è½½) --------
baseline_raw   = _fetch("model.pt")
baseline_model = _bytes_to_model(baseline_raw)

# -------- current_modelï¼ˆå¯çƒ­é‡è½½ï¼‰ --------
curr_raw = baseline_raw
model_sig = hashlib.md5(curr_raw).hexdigest()
t0_load = time.perf_counter()
with open("/mnt/pvc/models/model.pt", "wb") as f:
    f.write(curr_raw)
current_model = _bytes_to_model(curr_raw)
model_loading_ms = round((time.perf_counter() - t0_load) * 1000, 3)

def hot_reload():
    """è‹¥ MinIO ä¸Šçš„ model.pt å·²æ›´æ–°ï¼Œåˆ™é‡æ–°åŠ è½½å¹¶è®°å½•éƒ¨ç½²å»¶è¿Ÿ"""
    global current_model, model_sig, model_loading_ms
    raw = _fetch("model.pt")
    sig = hashlib.md5(raw).hexdigest()
    if sig == model_sig:
        return
    # 1. è®¡ç®—éƒ¨ç½²å»¶è¿Ÿ
    try:
        ts_bytes = _fetch("last_update_utc.txt")
        push_ts  = datetime.fromisoformat(ts_bytes.decode().strip("Z"))
        deploy_delay = (datetime.utcnow() - push_ts).total_seconds()
    except Exception:
        deploy_delay = None

    # 2. çƒ­é‡è½½
    t0 = time.perf_counter()
    current_model = _bytes_to_model(raw)
    model_sig     = sig
    model_loading_ms = round((time.perf_counter() - t0) * 1000, 3)
    with open("/mnt/pvc/models/model.pt", "wb") as f:
        f.write(raw)
    print(f"[infer] ğŸ”„ reloaded new model ({sig[:8]})  {model_loading_ms} ms")

    # 3. ä¸ŠæŠ¥
    log_metric(
        component="infer",
        event="hot_reload",
        model_loading_ms=model_loading_ms,
        deploy_delay_s=round(deploy_delay,3) if deploy_delay is not None else ""
    )

# -------- Kafka â†’ é˜Ÿåˆ— --------
q = queue.Queue()
def _listener():
    cons = KafkaConsumer(
        KAFKA_TOPIC,
        bootstrap_servers=",".join(KAFKA_SERVERS),
        group_id="cg-infer",
        auto_offset_reset=AUTO_OFFSET_RESET,
        enable_auto_commit=ENABLE_AUTO_COMMIT,
        value_deserializer=lambda m: json.loads(m.decode()),
    )
    for m in cons:
        q.put(m.value)

threading.Thread(target=_listener, daemon=True).start()

def _take_batch():
    buf, start = [], time.time()
    while len(buf) < BATCH_SIZE and time.time() - start < CONSUME_IDLE_S:
        try:
            buf.append(q.get(timeout=1))
        except queue.Empty:
            pass
    return buf

# ---------------- ä¸»å¾ªç¯ ----------------
perf = []  # æ”¶é›†æ‰€æœ‰æ‰¹æ¬¡çš„ metrics
rows, pred_orig_all, pred_adj_all = [], [], []
seen_any, first_batch = False, True
start_no_data = time.time()
MAX_WAIT_NO_DATA = 180
container_start = time.perf_counter()

print("[infer] consumer started â€¦")

while True:
    batch = _take_batch()
    if not batch:
        if not seen_any:
            if time.time() - start_no_data > MAX_WAIT_NO_DATA:
                print("[infer] no data for 3 min â†’ exit")
                break
            continue
        else:
            print("[infer] idle â†’ exit")
            break

    seen_any = True
    # å†·å¯åŠ¨ä¸ŠæŠ¥
    if first_batch:
        cold_ms = (time.perf_counter() - container_start) * 1000
        log_metric(component="infer", event="cold_start",
                   cold_start_ms=round(cold_ms,3))
        first_batch = False

    # è®¡ç®— RTT
    recv_ts = datetime.utcnow()
    rtts = []
    for r in batch:
        ts = r.get("send_ts")
        if ts:
            try:
                st = datetime.fromisoformat(ts.strip("Z"))
                rtts.append((recv_ts - st).total_seconds()*1000)
            except:
                pass
    avg_rtt = round(sum(rtts)/len(rtts),3) if rtts else None

    # æ„é€ ç‰¹å¾çŸ©é˜µ
    X = np.array([[r["features"].get(c,0.) for c in FEATURE_COLS] for r in batch], np.float32)
    Xs = scaler.transform(X)

    # æ¨ç†
    cpu0 = proc.cpu_times()
    t_start = time.perf_counter()
    with Timer("Inference", "infer"):
        with torch.no_grad():
            preds_adj  = current_model(torch.from_numpy(Xs).to(device)).cpu().numpy().ravel()
            preds_orig = baseline_model(torch.from_numpy(Xs).to(device)).cpu().numpy().ravel()
    cpu1 = proc.cpu_times()
    t_end = time.perf_counter()


    wall = t_end - t_start
    cpu_used = (cpu1.user + cpu1.system) - (cpu0.user + cpu0.system)
    cpu_pct  = round(100 * cpu_used / wall,2) if wall else 0.0
    gpu_pct  = (
        torch.cuda.memory_allocated() /
        torch.cuda.get_device_properties(0).total_memory * 100
        if device == "cuda" else 0.0
    )
    container_latency_ms = wall * 1000
    throughput_s = round(len(batch) / wall,3)

    # è®°å½•åˆ°æœ¬åœ°åˆ—è¡¨ perf
    entry = {
        "utc": datetime.utcnow().isoformat()+"Z",
        "batch_size": len(batch),
        "latency_ms": round(wall*1000,3),
        "cpu_pct": cpu_pct,
        "gpu_mem_pct": round(gpu_pct,2),
        "model_loading_ms": model_loading_ms,
        "container_latency_ms": round(container_latency_ms,3),
        "rtt_ms": avg_rtt or "",
        "throughput_s": throughput_s
    }
    perf.append(entry)

    print(f"[infer] batch {len(perf):03d} | {entry['latency_ms']}ms | "
          f"CPU {cpu_pct:.1f}% | RTT {avg_rtt}ms | TP {throughput_s}/s")

    # å†™æŒ‡æ ‡
    log_metric(
        component="infer",
        event="batch_metrics",
        **{k: v for k, v in entry.items() if k not in ("utc",)}
    )

    # æ”¶é›†é¢„æµ‹ï¼Œç”¨äºåç»­ä¿å­˜
    for r, yo, ya in zip(batch, preds_orig, preds_adj):
        r["pred_orig"], r["pred"] = float(yo), float(ya)
    rows.extend(batch)
    pred_orig_all.extend(preds_orig)
    pred_adj_all.extend(preds_adj)

    # æ£€æŸ¥å¹¶çƒ­é‡è½½æ¨¡å‹
    hot_reload()

# ---------- ä¿å­˜æ¨ç†ç»“æœ ----------
if rows:
    df = pd.DataFrame(rows)
    if "label" not in df.columns:
        df["label"] = np.nan

    # æ—§æ–‡ä»¶ï¼ˆå…¼å®¹ä»¥å‰è„šæœ¬ï¼‰
    np.save(f"/mnt/pvc/{RESULT_DIR}/inference_pred.npy",
            df["pred"].astype(np.float32).values)
    np.save(f"/mnt/pvc/{RESULT_DIR}/inference_true.npy",
            df["label"].astype(np.float32).values)
    save_np(f"{RESULT_DIR}/inference_pred.npy", df["pred"].values)
    save_np(f"{RESULT_DIR}/inference_true.npy", df["label"].values)

    # æ–°æ–‡ä»¶ï¼šorig / adj
    np.save(f"/mnt/pvc/{RESULT_DIR}/inference_pred_orig.npy",
            np.array(pred_orig_all, np.float32))
    np.save(f"/mnt/pvc/{RESULT_DIR}/inference_pred_adj.npy",
            np.array(pred_adj_all,  np.float32))
    save_np(f"{RESULT_DIR}/inference_pred_orig.npy", np.array(pred_orig_all))
    save_np(f"{RESULT_DIR}/inference_pred_adj.npy",  np.array(pred_adj_all))

# ---------- ä¿å­˜æ€§èƒ½æ—¥å¿— ----------
perf_path = f"/mnt/pvc/{RESULT_DIR}/timing/infer_perf.json"
with open(perf_path, "w") as fp:
    json.dump(perf, fp, indent=2)

save_bytes(f"{RESULT_DIR}/timing/infer_perf.json",
           json.dumps(perf, indent=2).encode(), "application/json")

print(f"[infer] DONE. rows={len(rows)}, perf_samples={len(perf)}")

# 2) ä¿å­˜ perf åˆ° CSV
df_perf = pd.DataFrame(perf)
csv_local = f"/mnt/pvc/{RESULT_DIR}/infer_metrics.csv"
df_perf.to_csv(csv_local, index=False)

# ä¸Šä¼ åˆ° MinIO
with open(csv_local, "rb") as fp:
    save_bytes(f"{RESULT_DIR}/infer_metrics.csv", fp.read(), "text/csv")

print(f"[infer] saved metrics CSV â†’ {csv_local}")
print(f"[infer] DONE. rows={len(rows)}, perf_samples={len(perf)}")
#!/usr/bin/env python3
"""
kafka_streaming/inference_consumer.py â€“ çœŸÂ·æµå¼æ¨ç†
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Extraction / Preprocessing / Inference_Engine ç”¨ Timer æ‰“ç‚¹
â€¢ æ¥ä¸€æ¡ Kafka æ¶ˆæ¯ç«‹å³æ¨ç†å¹¶åŸ‹ç‚¹
â€¢ Idle æ—¶ä¹Ÿ hot_reloadï¼›é€€å‡ºå‰å¼ºåˆ¶ hot_reload(sync)
â€¢ çƒ­æ›´ä¾æ® MinIO models/* + last_model_config.json
â€¢ æ‰“å°è¯¦ç»†é˜¶æ®µæ—¥å¿—ï¼Œæ–¹ä¾¿åœ¨ Kubeflow UI æŸ¥çœ‹
"""

from __future__ import annotations
import os, io, json, time, queue, threading, hashlib
from datetime import datetime
from collections import deque

import numpy as np
import joblib
import psutil
import torch
import torch.nn as nn
from kafka import KafkaConsumer

from shared.metric_logger import log_metric, sync_all_metrics_to_minio
from shared.profiler      import Timer
from shared.minio_helper  import s3, save_bytes, BUCKET
from shared.config        import (
    KAFKA_TOPIC, KAFKA_SERVERS, CONSUME_IDLE_S,
    MODEL_DIR, RESULT_DIR, AUTO_OFFSET_RESET, ENABLE_AUTO_COMMIT
)
from shared.features      import FEATURE_COLS
from ml.model             import DynamicMLP, build_model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0. åŸºç¡€ç¯å¢ƒä¸è·¯å¾„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
done_flag = f"/mnt/pvc/{RESULT_DIR}/producer_done.flag"
pod_name  = os.getenv("HOSTNAME", "infer")
local_out = f"/mnt/pvc/{RESULT_DIR}/{pod_name}"
os.makedirs(local_out, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
proc   = psutil.Process()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. MinIO æ‹‰å–å·¥å…·
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _fetch(key: str) -> bytes:
    return s3.get_object(Bucket=BUCKET, Key=f"{MODEL_DIR}/{key}")["Body"].read()

def _bytes_to_model(raw: bytes) -> nn.Module:
    """
    raw æ—¢å¯èƒ½æ˜¯æ•´æ¨¡å‹ï¼Œä¹Ÿå¯èƒ½æ˜¯ state_dictã€‚
    è‹¥ä¸º state_dictï¼Œåˆ™è¯»å– last_model_config.json åŠ¨æ€å»ºç½‘ç»œã€‚
    """
    obj = torch.load(io.BytesIO(raw), map_location=device)

    if isinstance(obj, nn.Module):
        return obj.to(device).eval()

    # state_dict â€“> éœ€è¦ç½‘ç»œç»“æ„
    try:
        cfg = json.loads(_fetch("last_model_config.json").decode())
    except Exception:
        cfg = None

    model = build_model(cfg, len(FEATURE_COLS)) if cfg else \
            DynamicMLP(len(FEATURE_COLS), (128, 64, 32))
    model.load_state_dict(obj)
    return model.eval()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. åŠ è½½ scaler & baseline model
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with open(f"{local_out}/scaler.pkl", "wb") as f:
    f.write(_fetch("scaler.pkl"))
scaler = joblib.load(f"{local_out}/scaler.pkl")

baseline_raw   = _fetch("model.pt")
baseline_model = _bytes_to_model(baseline_raw)

curr_raw          = baseline_raw
model_sig         = hashlib.md5(curr_raw).hexdigest()
t0_load           = time.perf_counter()
with open(f"{local_out}/model.pt", "wb") as f:
    f.write(curr_raw)
current_model     = _bytes_to_model(curr_raw)
model_loading_ms  = round((time.perf_counter() - t0_load) * 1000, 3)
print(f"[infer:{pod_name}] âœ“ model loaded ({model_loading_ms} ms)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Hot-reload æœºåˆ¶
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _reload_model(force: bool = False):
    """è‹¥è¿œç«¯æ¨¡å‹å˜æ›´åˆ™é‡æ–°åŠ è½½ï¼›force=True æ— æ¡ä»¶é‡è½½ã€‚"""
    global current_model, model_sig, model_loading_ms
    raw = _fetch("model.pt")
    sig = hashlib.md5(raw).hexdigest()

    if not force and sig == model_sig:
        return  # unchanged

    t0 = time.perf_counter()
    current_model = _bytes_to_model(raw)
    model_loading_ms = round((time.perf_counter() - t0) * 1000, 3)
    model_sig = sig
    with open(f"{local_out}/model.pt", "wb") as f:
        f.write(raw)

    log_metric(component="infer", event="hot_reload_runtime",
               model_loading_ms=model_loading_ms)
    print(f"[infer:{pod_name}] ğŸ”„ hot_reload â†’ {model_loading_ms} ms (force={force})")

def hot_reload():
    """å¼‚æ­¥è°ƒç”¨ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹ã€‚"""
    threading.Thread(target=_reload_model, daemon=True).start()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Kafka æ¶ˆè´¹çº¿ç¨‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
q: queue.Queue = queue.Queue()
producer_done = threading.Event()

def _listener():
    cons = KafkaConsumer(
        KAFKA_TOPIC,
        bootstrap_servers=",".join(KAFKA_SERVERS),
        group_id="cg-infer",
        auto_offset_reset=AUTO_OFFSET_RESET,
        enable_auto_commit=ENABLE_AUTO_COMMIT,
        value_deserializer=lambda m: json.loads(m.decode()),
    )
    # readiness flag
    flag = f"/mnt/pvc/{RESULT_DIR}/consumer_ready_{pod_name}.flag"
    with open(flag, "w"): pass
    print(f"[infer:{pod_name}] readiness flag â†’ {flag}")

    for msg in cons:
        data = msg.value
        data["_recv_ts"] = datetime.utcnow().isoformat() + "Z"
        if data.get("producer_done"):
            producer_done.set()
            continue
        q.put(data)

threading.Thread(target=_listener, daemon=True).start()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Forecasting Engineï¼ˆå¯é€‰ä»»åŠ¡ï¼Œæ­¤å¤„ç®€å•å‡å€¼ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
forecast_hist = deque(maxlen=300)
def _forecast_loop():
    while True:
        time.sleep(30)
        if not forecast_hist:
            continue
        with Timer("Forecasting_Engine", "infer"):
            _ = float(np.mean(forecast_hist))
        log_metric(component="infer", event="forecasting_runtime")
        print(f"[infer:{pod_name}] Forecasting_Engine run, hist_len={len(forecast_hist)}")

threading.Thread(target=_forecast_loop, daemon=True).start()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ä¸»å¾ªç¯ï¼šé€æ¡æ¨ç†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
IDLE_TIMEOUT_S  = 180
container_start = time.perf_counter()
first_msg       = True
last_data_ts    = time.time()

print(f"[infer:{pod_name}] streaming loop start â€¦")

while True:
    # ----- æ‹‰å– 1 æ¡ -----
    try:
        row = q.get(timeout=CONSUME_IDLE_S)
    except queue.Empty:
        hot_reload()  # ç©ºé—²ä¹Ÿè¯•è¯•çƒ­æ›´æ–°
        now = time.time()

        # é€€å‡ºåˆ¤å®š 1ï¼šProducer å®Œæˆ
        if producer_done.is_set() or os.path.exists(done_flag):
            print(f"[infer:{pod_name}] producer_done detected â†’ final hot_reload & exit")
            _reload_model(force=True)
            break

        # é€€å‡ºåˆ¤å®š 2ï¼šç©ºé—²è¶…æ—¶
        if now - last_data_ts > IDLE_TIMEOUT_S:
            print(f"[infer:{pod_name}] idle>{IDLE_TIMEOUT_S}s â†’ final hot_reload & exit")
            _reload_model(force=True)
            break
        continue

    last_data_ts = time.time()

    # ----- Cold-start æ‰“ç‚¹ -----
    if first_msg:
        cold_ms = (time.perf_counter() - container_start) * 1000
        log_metric(component="infer", event="cold_start",
                   cold_start_ms=round(cold_ms, 3))
        print(f"[infer:{pod_name}] cold_start â†’ {cold_ms:.2f} ms")
        first_msg = False

    # ==========  A) Extraction ==========
    with Timer("Extraction", "infer"):
        X_raw = np.array([[row["features"].get(c, 0.0) for c in FEATURE_COLS]],
                         dtype=np.float32)
    print(f"[infer:{pod_name}] Extraction done")

    # ==========  B) Preprocessing =======
    with Timer("Preprocessing", "infer"):
        X_scaled = scaler.transform(X_raw)
    print(f"[infer:{pod_name}] Preprocessing done")

    # ==========  C) Inference_Engine ====
    cpu0, t0 = proc.cpu_times(), time.perf_counter()
    with Timer("Inference_Engine", "infer"):
        with torch.no_grad():
            pred_adj  = current_model(torch.from_numpy(X_scaled).to(device)).cpu().item()
            pred_orig = baseline_model(torch.from_numpy(X_scaled).to(device)).cpu().item()
    cpu1, t1 = proc.cpu_times(), time.perf_counter()
    print(f"[infer:{pod_name}] Inference_Engine done")

    # ==========  D) æŒ‡æ ‡ & æ—¥å¿— ==========
    wall_ms  = (t1 - t0) * 1000
    cpu_used = ((cpu1.user + cpu1.system) - (cpu0.user + cpu0.system)) * 1000
    cpu_pct  = round(cpu_used / wall_ms, 2) if wall_ms else 0.0

    rtt_ms = 0.0
    if "send_ts" in row:
        try:
            st = datetime.fromisoformat(row["send_ts"].rstrip("Z"))
            rt = (datetime.fromisoformat(row["_recv_ts"].rstrip("Z")) - st).total_seconds() * 1000
            rtt_ms = round(rt, 3)
        except Exception:
            pass

    log_metric(
        component="infer",
        event="single_metrics",
        latency_ms=round(wall_ms, 3),
        cpu_pct=cpu_pct,
        rtt_ms=rtt_ms,
        model_loading_ms=model_loading_ms,
        gpu_mem_pct=0.0
    )
    print(f"[infer:{pod_name}] single_metrics: "
          f"lat={wall_ms:.1f} ms | rtt={rtt_ms:.1f} ms | cpu={cpu_pct}%")

    # æ”¶é›†é¢„æµ‹å†å²
    forecast_hist.append(pred_adj)

    # å¼‚æ­¥çƒ­æ›´æ–°
    hot_reload()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. é€€å‡ºæ”¶å°¾
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
global_csv = f"/mnt/pvc/{RESULT_DIR}/metrics_summary.csv"
if os.path.exists(global_csv):
    with open(global_csv, "rb") as f:
        save_bytes(f"{RESULT_DIR}/{pod_name}_infer_metrics.csv",
                   f.read(), "text/csv")
    print(f"[infer:{pod_name}] metrics CSV uploaded as {pod_name}_infer_metrics.csv")

sync_all_metrics_to_minio()
print(f"[infer:{pod_name}] all metrics synced â€“ bye.")

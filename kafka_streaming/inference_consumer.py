#!/usr/bin/env python3
"""
kafka_streaming/inference_consumer.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
baseline  : offline è®­ç»ƒå¾—åˆ°çš„ MLP Scalerâ†’PCAâ†’N ç»´è¾“å…¥
adaptive  : çƒ­æ›´æ–°æ¨¡å‹ Scaler åç›´æ¥åƒ full_dim ç‰¹å¾
ä¸¤è·¯é¢„æµ‹ + çœŸå®å€¼ å…¨é‡è®°å½•ï¼Œä¾› plot_final.py ç›´æ¥æ‹¼ Phase-1/2/3
æ‰€æœ‰åŸæœ‰æŒ‡æ ‡åŸ‹ç‚¹ ,æ—¥å¿—å®Œå…¨ä¿ç•™
"""

import os, io, json, time, queue, threading, hashlib
from datetime import datetime
from collections import deque
from typing import List

import threading
import numpy as np
import joblib
import psutil
import torch
import torch.nn as nn
from kafka import KafkaConsumer, KafkaProducer
from shared.config import CONTROL_TOPIC, KAFKA_TOPIC, KAFKA_SERVERS
from shared.utils import _fetch, _bytes_to_model
from shared.metric_logger import log_metric, sync_all_metrics_to_minio
from shared.profiler      import Timer
from shared.minio_helper  import s3, save_bytes, save_np, BUCKET
from shared.config        import (
    KAFKA_TOPIC, KAFKA_SERVERS, BATCH_SIZE, CONSUME_IDLE_S,
    MODEL_DIR, RESULT_DIR, AUTO_OFFSET_RESET, ENABLE_AUTO_COMMIT
)
from shared.features      import FEATURE_COLS
from ml.model             import DynamicMLP, build_model

from botocore.exceptions import ClientError    
 
pause_event = threading.Event()
# ---------- å¸¸é‡ & æœ¬åœ°è·¯å¾„ ---------------------------------------------

RETRAIN_TOPIC = os.getenv("RETRAIN_TOPIC", KAFKA_TOPIC + "_infer_count")   # kafka topic total 100
model_lock = threading.Lock()
MODEL_IMPROVE_EPS = float(os.getenv("MODEL_IMPROVE_EPS", "1.0"))  # %

TMP_DIR  = "/tmp/infer"                     # â† æ”¹ç”¨ /tmp
os.makedirs(TMP_DIR, exist_ok=True)

pod_name = os.getenv("HOSTNAME", "infer")   # k8s å®¹å™¨å

local_out = os.path.join(TMP_DIR, pod_name) # æ¯ä¸ª consumer ç‹¬ç«‹ç›®å½•
os.makedirs(local_out, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
proc   = psutil.Process()

pred_orig_hist: List[float] = []
pred_adj_hist : List[float] = []
true_hist     : List[float] = []
ts_hist       : List[str]   = []

# ---------- é€‰ç‰¹å¾ & scaler åŠ è½½ ------------------------------------------
import json, io, joblib

# 1) æ‹‰å–å¹¶åŠ è½½ä¸ output_rate æœ€ç›¸å…³çš„ top10 ç‰¹å¾åˆ—è¡¨
raw_feats = _fetch("selected_feats.json")
SELECTED_FEATS = json.loads(raw_feats)
print(f"[infer:{pod_name}] using selected feats: {SELECTED_FEATS}")

# 2) æ‹‰å–å¹¶åŠ è½½å¯¹åº”çš„ StandardScaler
with open(f"{local_out}/scaler.pkl", "wb") as f:
    f.write(_fetch("scaler.pkl"))
scaler = joblib.load(f"{local_out}/scaler.pkl")


# ---------- å®Œæ•´æ¨¡å‹åŠ è½½å·¥å…·å‡½æ•° ------------------------------------------
def _load_full_model(key: str) -> tuple[nn.Module, bytes]:
    """
    ä» MinIO æ‹‰å– key å¯¹åº”æ–‡ä»¶ï¼Œç¡®ä¿å®ƒæ˜¯å®Œæ•´ torch.save(model) å¯¼å‡ºçš„ nn.Moduleã€‚
    å¦‚æœæ”¶åˆ° OrderedDictï¼ˆstate_dictï¼‰ï¼Œ_bytes_to_model ä¼šæŠ› TypeErrorã€‚
    è¿”å› (model, raw_bytes)ã€‚
    """
    raw = _fetch(key)
    model = _bytes_to_model(raw).to(device)  # _bytes_to_model å·²ç» .eval()
    return model, raw

# ---------- baseline & adaptive æ¨¡å‹è£…è½½ ----------------------------------
baseline_model, base_raw = _load_full_model("baseline_model.pt")
baseline_in_dim          = baseline_model.net[0].in_features

current_model, curr_raw  = _load_full_model("model.pt")
current_model._val_acc15 = 0.0          # åˆå§‹åŸºçº¿

print(f"[infer:{pod_name}] baseline in_features = {baseline_in_dim}")
print(f"[infer:{pod_name}] adaptive model       = {current_model}")

model_sig        = hashlib.md5(curr_raw).hexdigest()
model_loading_ms = 0.0


# ---------- çƒ­é‡è½½ --------------------------------------------------------
GAIN_THR_PP = float(os.getenv("GAIN_THRESHOLD_PP", "0.01"))  # â‰¥0.x ä¸ªç™¾åˆ†ç‚¹å°±æ¢

# ---------- çƒ­é‡è½½ --------------------------------------------------------
def _reload_model(force: bool = False):
    """
    çƒ­åŠ è½½ï¼šä»…å½“æ–°æ¨¡å‹ç›¸å¯¹ baseline ç²¾åº¦æå‡ >= GAIN_THR_PP
    æˆ–è€… force=True æ—¶æ‰æ›¿æ¢ current_modelã€‚
    ğŸ”’ çº¿ç¨‹å®‰å…¨ï¼šæ›´æ–°è¿‡ç¨‹æŒ model_lockã€‚
    """
    global current_model, curr_raw, model_sig, model_loading_ms

    try:
        # 1) å°è¯•æ‹‰å– latest.txt
        latest_raw = _fetch("latest.txt")
        model_key, metrics_key = latest_raw.decode().strip().splitlines()

        # 2) æ‹‰å–æ¨¡å‹å­—èŠ‚æµå¹¶è®¡ç®—ç­¾å
        raw = _fetch(model_key)
        sig = hashlib.md5(raw).hexdigest()

        # 3) éå¼ºåˆ¶ä¸”ç­¾åæœªå˜ â†’ è·³è¿‡
        if not force and sig == model_sig:
            return

        # 4) è¯»å–éªŒè¯é›†å‡†ç¡®ç‡ï¼Œè®¡ç®—å¢ç›Š
        metrics = json.loads(_fetch(metrics_key).decode())
        new_acc  = metrics.get("acc@0.15",    0.0)
        base_acc = metrics.get("baseline_acc@0.15", 0.0)
        gain_pp  = new_acc - base_acc

        # 5) éå¼ºåˆ¶ä¸”å¢ç›Šä¸è¶³ â†’ è·³è¿‡
        if not force and gain_pp < GAIN_THR_PP:
            print(f"[infer:{pod_name}] Î”{gain_pp:+.3f} pp < {GAIN_THR_PP} â†’ skip reload")
            return

        # 6) åŠ è½½æ–°æ¨¡å‹å¹¶åŸå­æ›´æ–°
        t0 = time.perf_counter()
        mdl = torch.load(io.BytesIO(raw), map_location=device).eval()
        with model_lock:
            current_model      = mdl
            current_model._val_acc15 = new_acc
            curr_raw, model_sig     = raw, sig
        model_loading_ms = round((time.perf_counter() - t0) * 1000, 3)

        log_metric(component="infer", event="hot_reload_runtime",
                   model_loading_ms=model_loading_ms)
        print(
            f"[infer:{pod_name}] hot-reloaded âœ“  "
            f"baseline={base_acc:.2f}% â†’ new={new_acc:.2f}%  "
            f"(Î”{gain_pp:+.3f} pp)  load={model_loading_ms} ms"
        )

    except ClientError as e:
        # latest.txt ä¸å­˜åœ¨ â†’ è·³è¿‡ï¼›å…¶ä»– S3 é”™è¯¯éƒ½ log åè·³è¿‡
        code = e.response.get("Error", {}).get("Code", "")
        if code == "NoSuchKey":
            print(f"[infer:{pod_name}] reload: no latest.txt â†’ skip")
        else:
            print(f"[infer:{pod_name}] reload ClientError â†’ {e}")
        return

    except Exception as e:
        # ç½‘ç»œä¸­æ–­ã€è¿æ¥è¢«æ‹’ç­‰ä¸€åˆ‡å¼‚å¸¸éƒ½ log åè·³è¿‡
        print(f"[infer:{pod_name}] reload unexpected error â†’ {e}")
        return



# ---------- Kafka ç›‘å¬çº¿ç¨‹ï¼ˆå¸¦é‡è¯•ï¼‰ -----------------------------------------------
import time
from kafka.errors import NoBrokersAvailable

q = queue.Queue()
producer_done = threading.Event()

def _create_consumer():
    for attempt in range(1, 11):
        try:
            cons = KafkaConsumer(
                KAFKA_TOPIC, CONTROL_TOPIC,
                bootstrap_servers=",".join(KAFKA_SERVERS),
                group_id="cg-infer",
                auto_offset_reset=AUTO_OFFSET_RESET,
                enable_auto_commit=ENABLE_AUTO_COMMIT,
                value_deserializer=lambda m: json.loads(m.decode()),
                api_version_auto_timeout_ms=10000,
            )
            return cons
        except NoBrokersAvailable:
            print(f"[infer:{pod_name}] bootstrap brokers unavailable ({attempt}/10), retrying in 5sâ€¦")
            time.sleep(5)
    raise RuntimeError("[infer] Kafka still unreachable after 10 retries")

def _listener():
    # 0) ç­‰å¾… producer_ready.flag ï¼ˆé˜²æ­¢æŠ¢è·‘ï¼‰
    MAX_WAIT_FOR_PRODUCER = 600
    WAIT_STEP = 5
    flag_key = f"{RESULT_DIR}/producer_ready.flag"

    def _wait_for_producer_flag():
        for i in range(0, MAX_WAIT_FOR_PRODUCER, WAIT_STEP):
            try:
                s3.head_object(Bucket=BUCKET, Key=flag_key)
                print(f"[infer:{pod_name}] âœ“ detected producer_ready.flag in MinIO")
                return
            except ClientError as e:
                if e.response["Error"]["Code"] != "404":
                    print(f"[infer:{pod_name}] MinIO error â†’ {e}")
                    break
            time.sleep(WAIT_STEP)
        raise TimeoutError(f"[infer:{pod_name}] âŒ wait for producer_ready.flag timeout after {MAX_WAIT_FOR_PRODUCER}s")

    _wait_for_producer_flag()

    # 1) åˆ›å»º consumerï¼ˆå†…å«é‡è¯•ï¼‰
    cons = _create_consumer()
    print(f"[infer:{pod_name}] KafkaConsumer created, beginning to pollâ€¦")

    # 2) readiness flagï¼š/tmp + MinIO
    flag_local = os.path.join(TMP_DIR, f"consumer_ready_{pod_name}.flag")
    open(flag_local, "w").close()
    print(f"[infer:{pod_name}] readiness flag â†’", flag_local)
    save_bytes(f"{RESULT_DIR}/consumer_ready_{pod_name}.flag",
               b"", "text/plain")

    # 3) æŒç»­æ¥æ”¶æ¶ˆæ¯
    for msg in cons:
        v = msg.value
        # â€”â€” æ§åˆ¶æ¶ˆæ¯ â€”â€” 
        if "retrain" in v:
            if v["retrain"] == "start":
                print("[infer] PAUSE consumption for retrain")
                pause_event.set()
            elif v["retrain"] == "end":
                print("[infer] RESUME consumption after retrain")
                pause_event.clear()
            continue
        if v.get("producer_done"):
            producer_done.set()
            continue
        # æ³¨å…¥æ¥æ”¶æ—¶é—´æˆ³
        v["_recv_ts"] = datetime.utcnow().isoformat() + "Z"
        q.put(v)


# -----------------------------------------------------------------------------
# å‘¨æœŸæ€§åå°çƒ­é‡è½½ï¼ˆæ¯ RELOAD_INTERVAL_S ç§’è°ƒç”¨ä¸€æ¬¡ _reload_modelï¼‰
# -----------------------------------------------------------------------------
RELOAD_INTERVAL_S = int(os.getenv("RELOAD_INTERVAL_S", "30"))

def _reload_daemon():
    while True:
        time.sleep(RELOAD_INTERVAL_S)
        try:
            _reload_model()
        except Exception as e:
            # ç†è®ºä¸Š _reload_model å·²ç»åæ‰æ‰€æœ‰å¼‚å¸¸ï¼Œè¿™é‡Œåšå…œåº•
            print(f"[infer:{pod_name}] reload daemon error â†’ {e}")

# å¯åŠ¨å®ˆæŠ¤çº¿ç¨‹
threading.Thread(target=_reload_daemon, daemon=True).start()

# åºŸå¼ƒåŸ per-batch çƒ­é‡è½½ï¼Œå°† hot_reload æ”¹ä¸ºç©ºå®ç°
def hot_reload():
    # æ¯ä¸ª batch ä¸å†è§¦å‘çƒ­é‡è½½ï¼Œæ”¹ç”¨ä¸Šé¢çš„å®ˆæŠ¤çº¿ç¨‹
    pass
# -----------------------------------------------------------------------------

def _take_batch():
    buf = []
    try: buf.append(q.get(timeout=CONSUME_IDLE_S))
    except queue.Empty: return buf
    while len(buf) < BATCH_SIZE:
        try: buf.append(q.get_nowait())
        except queue.Empty: break
    return buf


trigger_prod = KafkaProducer(
    bootstrap_servers=",".join(KAFKA_SERVERS),
    value_serializer=lambda m: json.dumps(m).encode(),
)

# ---------- Forecasting (demo) -------------------------------------------
forecast_hist = deque(maxlen=300)
def _forecast_loop():
    while True:
        time.sleep(30)
        if not forecast_hist: continue
        with Timer("Forecasting_Engine", "infer"): _ = float(np.mean(forecast_hist))
        log_metric(component="infer", event="forecasting_runtime")
threading.Thread(target=_forecast_loop, daemon=True).start()


def _align_to_dim(X_scaled: np.ndarray, in_dim: int) -> np.ndarray:
    """
    æŠŠ 60-ç»´ Scaler ç‰¹å¾è£å‰ª/è¡¥é›¶åˆ°ç›®æ ‡ in_dimã€‚
    """
    if in_dim == X_scaled.shape[1]:
        return X_scaled
    elif in_dim < X_scaled.shape[1]:
        return X_scaled[:, :in_dim]          # æˆªæ–­
    else:
        pad = np.zeros((X_scaled.shape[0], in_dim - X_scaled.shape[1]),
                       dtype=np.float32)
        return np.concatenate([X_scaled, pad], axis=1)


# ---------------------------  ä¸»å¾ªç¯  ------------------------------------
first_batch     = True
container_start = time.perf_counter()
IDLE_TIMEOUT_S  = 180
last_data_time  = time.time()
msg_total       = 0
correct_count   = 0
total_count     = 0

print(f"[infer:{pod_name}] consumer startedâ€¦")

while True:
    # å¦‚æœæ­£åœ¨ retrainï¼Œåˆ™é˜»å¡åœ¨è¿™é‡Œï¼Œç›´åˆ°æ”¶åˆ° resume ä¿¡å·
    while pause_event.is_set():
        time.sleep(1)
    batch = _take_batch()
    now   = time.time()
    
    if not batch:  #  åªåœ¨ â€œæ”¶åˆ° producer_doneâ€ ä¸” â€œæœ¬åœ°é˜Ÿåˆ—å·²æ¸…ç©ºâ€ æ—¶æ‰ä¼˜é›…æ”¶å°¾
        if producer_done.is_set() and q.empty():
            try:
                _reload_model(force=True)   # æœ€åå†å¼ºåˆ¶è½½ä¸€æ¬¡æœ€æ–°æ¨¡å‹
            except Exception as e:
                print(f"[infer:{pod_name}] final reload error â†’ {e}")
            print(f"[infer:{pod_name}] graceful shutdown")
            break
        time.sleep(0.3)
        continue

    last_data_time = now
    if first_batch:
        cold_ms = (time.perf_counter() - container_start) * 1000
        log_metric(component="infer", event="cold_start",
                   cold_start_ms=round(cold_ms, 3))
        first_batch = False

    # 1) Extraction --------------------------------------------------------
    with Timer("Extraction", "infer"):
        rows_batch = list(batch)

    # 2) Preprocessing -----------------------------------------------------
    with Timer("Preprocessing", "infer"):
        # â‘  æå– top-10 åŸå§‹ç‰¹å¾
        X_raw = np.array(
            [[r["features"].get(c, 0.0) for c in SELECTED_FEATS]
             for r in rows_batch],
            dtype=np.float32
        )
        # â‘¡ æ ‡å‡†åŒ–
        X_scaled = scaler.transform(X_raw)

    # â”€â”€ çº¿ç¨‹å®‰å…¨åœ°æ‹¿ä¸€ä»½å½“å‰ adaptive æ¨¡å‹å¼•ç”¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with model_lock:
        model_ref = current_model          # åªåœ¨ä¸´ç•ŒåŒºåšå¼•ç”¨æ‹·è´

    # === å…³é”®ï¼šè®©ä¸¤è·¯è¾“å…¥ä¸å„è‡ªæ¨¡å‹é¦–å±‚ç»´åº¦åŒ¹é… ============================
    X_base = _align_to_dim(X_scaled, baseline_in_dim)   # åŸºçº¿æ¨¡å‹å›ºå®š 10 ç»´
    X_adpt = _align_to_dim(X_scaled, model_ref.net[0].in_features)
        
        
    # 3) Inference ---------------------------------------------------------
    cpu0, t0 = proc.cpu_times(), time.perf_counter()
    with Timer("Inference_Engine", "infer"):
        with torch.no_grad():
            # Baseline é¢„æµ‹
            preds_base = baseline_model(
                torch.from_numpy(X_base).to(device)
            ).cpu().numpy().ravel()
            # Adaptive é¢„æµ‹ï¼ˆä½¿ç”¨åˆšæ‰çº¿ç¨‹å®‰å…¨å–åˆ°çš„ model_refï¼‰
            preds_adpt = model_ref(
                torch.from_numpy(X_adpt).to(device)
            ).cpu().numpy().ravel()
    cpu1, t1 = proc.cpu_times(), time.perf_counter()

    # 4) Accuracy@0.2 ç»Ÿè®¡
    labels = np.array([r["label"] for r in rows_batch], np.float32)
    errs   = np.abs(preds_adpt - labels) / np.maximum(labels, 1e-8)
    batch_correct = int((errs <= 0.2).sum())
    batch_total   = len(labels)
    correct_count += batch_correct
    total_count   += batch_total
    cum_acc = correct_count / total_count
    print(f"[infer:{pod_name}] accuracy@0.2 â†’ batch {batch_correct}/{batch_total}, "
          f"cumulative {correct_count}/{total_count} = {cum_acc:.3f}")
    log_metric(
        component="infer",
        event="cumulative_accuracy",
        threshold=0.2,
        batch_correct=batch_correct,
        batch_total=batch_total,
        cumulative_correct=correct_count,
        cumulative_total=total_count,
        cumulative_accuracy=round(cum_acc, 3)
    )
    
    # ---------- DEBUGï¼šè¯¯å·®åˆ†å¸ƒï¼ˆåŸºçº¿ vs. è‡ªé€‚åº”ï¼‰ ----------
    # â‘  ç›¸å¯¹è¯¯å·®
    err_base = np.abs(preds_base - labels) / np.maximum(labels, 1e-8)
    err_adpt = np.abs(preds_adpt - labels) / np.maximum(labels, 1e-8)

    # â‘¡ å…³é”®åˆ†ä½æ•°
    pct = [50, 80, 90, 95, 99]
    base_q = np.percentile(err_base, pct).round(3)
    adpt_q = np.percentile(err_adpt, pct).round(3)

    print(
        f"[infer:{pod_name}]  Î”(relative) | "
        f"BASE p50/p80/p90/p95/p99 = {base_q.tolist()} | "
        f"ADPT = {adpt_q.tolist()}"
    )

    log_metric(
        component="infer",
        event="err_dist",
        base_p50=float(base_q[0]), base_p80=float(base_q[1]),
        base_p90=float(base_q[2]), base_p95=float(base_q[3]),
        base_p99=float(base_q[4]),
        adpt_p50=float(adpt_q[0]), adpt_p80=float(adpt_q[1]),
        adpt_p90=float(adpt_q[2]), adpt_p95=float(adpt_q[3]),
        adpt_p99=float(adpt_q[4]),
    )
    
    # ---------- æ”¶é›†æŒ‡æ ‡ & æ—¥å¿— ------------------------------------------
    ts_hist.extend([r["send_ts"] for r in rows_batch])
    # 5) åŸæœ‰æŒ‡æ ‡ & æ—¥å¿—
    pred_orig_hist.extend(preds_base.tolist())
    pred_adj_hist .extend(preds_adpt.tolist())
    true_hist     .extend(labels.tolist())

    # è®¡ç®— RTTã€CPU%ã€TPS ç­‰
    rtts = []
    for r in rows_batch:
        if "send_ts" not in r:
            continue
        try:
            st = datetime.fromisoformat(r["send_ts"].rstrip("Z"))
            rt = (datetime.fromisoformat(r["_recv_ts"].rstrip("Z")) - st
                  ).total_seconds() * 1000
            rtts.append(rt)
        except Exception:
            pass
    avg_rtt = round(np.mean(rtts), 3) if rtts else 0.0
    wall_ms = (t1 - t0) * 1000
    cpu_used = ((cpu1.user + cpu1.system) - (cpu0.user + cpu0.system)) * 1000
    cpu_pct = round(cpu_used / wall_ms, 2) if wall_ms else 0.0
    tp_s    = round(len(rows_batch) / (wall_ms or 1e-3), 3)

    log_metric(
        component="infer", event="batch_metrics",
        batch_size=len(rows_batch),
        latency_ms=round(wall_ms, 3),
        throughput_s=tp_s,
        cpu_pct=cpu_pct,
        gpu_mem_pct=0.0,
        model_loading_ms=model_loading_ms,
        container_latency_ms=round(wall_ms, 3),
        rtt_ms=avg_rtt
    )
    msg_total += len(rows_batch)
    print(f"[infer:{pod_name}] batch_metrics: "
          f"msg_total={msg_total}, latency={wall_ms:.3f}ms, "
          f"avg_rtt={avg_rtt}ms, cpu_pct={cpu_pct}/s")
    try:
        trigger_prod.send(RETRAIN_TOPIC, {"processed": batch_total})
    except Exception:
        pass
    # 6) Forecast & Hot-reload ä¿æŒä¸å˜
    forecast_hist.extend(preds_adpt)
    hot_reload()

# 7) æ”¶å°¾ï¼šä¿å­˜æ•°ç»„ & ä¸Šä¼ 
print(f"[infer:{pod_name}] TOTAL processed {msg_total} samples, exit")


# ---------- æ”¶å°¾ï¼šä¿å­˜å®Œæ•´ trace å¹¶ä¸Šä¼  ----------------------------------
print(f"[infer:{pod_name}] TOTAL processed {msg_total} samples, exit")

arr_adj   = np.asarray(pred_adj_hist , np.float32)
arr_orig  = np.asarray(pred_orig_hist, np.float32)
arr_true  = np.asarray(true_hist     , np.float32)
arr_ts    = np.asarray([
    datetime.fromisoformat(t.rstrip("Z")).timestamp()
    for t in ts_hist
], np.float64)

npz_local = os.path.join(local_out, "inference_trace.npz")
np.savez(npz_local,     # ä¿å­˜ä¸º npz æ ¼å¼ æ•°æ®
         ts=arr_ts,     # æ¯æ¡æ ·æœ¬çš„å‘é€æ—¶é—´æˆ³
         pred_adj=arr_adj,  # çƒ­æ›´æ–°æ¨¡å‹çš„é¢„æµ‹å€¼åºåˆ—
         pred_orig=arr_orig,  # åŸºçº¿æ¨¡å‹çš„é¢„æµ‹å€¼åºåˆ—
         true=arr_true)  # çœŸå®æ ‡ç­¾åºåˆ—, ç”¨æ¥ä¹‹åç®—ååé‡

with open(npz_local, "rb") as f:
    save_bytes(f"{RESULT_DIR}/{pod_name}_inference_trace.npz",
               f.read(), "application/octet-stream")
print(f"[infer:{pod_name}] trace npz saved â€“ total {len(arr_true)} samples")

sync_all_metrics_to_minio()
print(f"[infer:{pod_name}] all metrics synced, exiting.")



#!/usr/bin/env python3
"""
kafka_streaming/inference_consumer.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
baseline  : offline è®­ç»ƒå¾—åˆ°çš„ MLP Scalerâ†’PCAâ†’N ç»´è¾“å…¥
adaptive  : çƒ­æ›´æ–°æ¨¡å‹ Scaler åç›´æ¥åƒ full_dim ç‰¹å¾
ä¸¤è·¯é¢„æµ‹ + çœŸå®å€¼ å…¨é‡è®°å½•ï¼Œä¾› plot_final.py ç›´æ¥æ‹¼ Phase-1/2/3
æ‰€æœ‰åŸæœ‰æŒ‡æ ‡åŸ‹ç‚¹ ,æ—¥å¿—å®Œå…¨ä¿ç•™
"""

import os, io, json, time, queue, threading, hashlib
from datetime import datetime
from collections import deque
from typing import List

import threading
import numpy as np
import joblib
import psutil
import torch
import torch.nn as nn
from kafka import KafkaConsumer, KafkaProducer
from kafka.errors import NoBrokersAvailable
from shared.utils import _fetch, _bytes_to_model
from shared.metric_logger import log_metric, sync_all_metrics_to_minio
from shared.profiler      import Timer
from shared.minio_helper  import s3, save_bytes, save_np, BUCKET
from shared.config        import (
    KAFKA_TOPIC, KAFKA_SERVERS, BATCH_SIZE, CONSUME_IDLE_S,
    MODEL_DIR, RESULT_DIR, AUTO_OFFSET_RESET, ENABLE_AUTO_COMMIT
)
from shared.features      import FEATURE_COLS
from ml.model             import DynamicMLP, build_model
from botocore.exceptions import ClientError     

# ---------- å¸¸é‡ & æœ¬åœ°è·¯å¾„ ---------------------------------------------

# â€”â€” ç†”æ–­ç›¸å…³å¸¸é‡ â€”â€” 
ACC_WINDOW    = 300      # è¦å’Œ monitor.WINDOW_SIZE ä¿æŒä¸€è‡´
ACC_THRESHOLD = 0.50     # é˜ˆå€¼ï¼šå½“æ»‘çª—å‡†ç¡®ç‡ < 50% æ—¶ç®—ä¸€æ¬¡ä½å‡†ç¡®
LOW_MAX       = 2        # è¿ç»­ 2 ä¸ªæ»‘çª—éƒ½ä½å‡†ç¡®æ‰è§¦

RETRAIN_TOPIC = os.getenv("RETRAIN_TOPIC", KAFKA_TOPIC + "_infer_count")   # kafka topic total 100
model_lock = threading.Lock()
MODEL_IMPROVE_EPS = float(os.getenv("MODEL_IMPROVE_EPS", "1.0"))  # %

TMP_DIR  = "/tmp/infer"                     # â† æ”¹ç”¨ /tmp
os.makedirs(TMP_DIR, exist_ok=True)

pod_name = os.getenv("HOSTNAME", "infer")   # k8s å®¹å™¨å

local_out = os.path.join(TMP_DIR, pod_name) # æ¯ä¸ª consumer ç‹¬ç«‹ç›®å½•
os.makedirs(local_out, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
proc   = psutil.Process()

pred_orig_hist: List[float] = []
pred_adj_hist : List[float] = []
true_hist     : List[float] = []
ts_hist       : List[str]   = []

# ---------- é€‰ç‰¹å¾ & scaler åŠ è½½ ------------------------------------------
import json, io, joblib

# 1) æ‹‰å–å¹¶åŠ è½½ä¸ output_rate æœ€ç›¸å…³çš„ top10 ç‰¹å¾åˆ—è¡¨
raw_feats = _fetch("selected_feats.json")
SELECTED_FEATS = json.loads(raw_feats)
print(f"[infer:{pod_name}] using selected feats: {SELECTED_FEATS}")

# 2) æ‹‰å–å¹¶åŠ è½½å¯¹åº”çš„ StandardScaler
with open(f"{local_out}/scaler.pkl", "wb") as f:
    f.write(_fetch("scaler.pkl"))
scaler = joblib.load(f"{local_out}/scaler.pkl")


# ---------- å®Œæ•´æ¨¡å‹åŠ è½½å·¥å…·å‡½æ•° ------------------------------------------
def _load_full_model(key: str) -> tuple[nn.Module, bytes]:
    """
    ä» MinIO æ‹‰å– key å¯¹åº”æ–‡ä»¶ï¼Œç¡®ä¿å®ƒæ˜¯å®Œæ•´ torch.save(model) å¯¼å‡ºçš„ nn.Moduleã€‚
    å¦‚æœæ”¶åˆ° OrderedDictï¼ˆstate_dictï¼‰ï¼Œ_bytes_to_model ä¼šæŠ› TypeErrorã€‚
    è¿”å› (model, raw_bytes)ã€‚
    """
    raw = _fetch(key)
    model = _bytes_to_model(raw).to(device)  # _bytes_to_model å·²ç» .eval()
    return model, raw

# ---------- baseline & adaptive æ¨¡å‹è£…è½½ ----------------------------------
baseline_model, base_raw = _load_full_model("baseline_model.pt")
baseline_in_dim          = baseline_model.net[0].in_features

current_model, curr_raw  = _load_full_model("model.pt")
current_model._val_acc15 = 0.0          # åˆå§‹åŸºçº¿

print(f"[infer:{pod_name}] baseline in_features = {baseline_in_dim}")
print(f"[infer:{pod_name}] adaptive model       = {current_model}")

model_sig        = hashlib.md5(curr_raw).hexdigest()
model_loading_ms = 0.0


# ---------- çƒ­é‡è½½ --------------------------------------------------------
GAIN_THR_PP = float(os.getenv("GAIN_THRESHOLD_PP", "0.01"))  # â‰¥0.x ä¸ªç™¾åˆ†ç‚¹å°±æ¢

# ---------- çƒ­é‡è½½ --------------------------------------------------------
def _reload_model(force: bool = False):
    """
    çƒ­åŠ è½½ï¼šä»…å½“æ–°æ¨¡å‹ç›¸å¯¹ baseline ç²¾åº¦æå‡ >= GAIN_THR_PP
    æˆ–è€… force=True æ—¶æ‰æ›¿æ¢ current_modelã€‚
    ğŸ”’ çº¿ç¨‹å®‰å…¨ï¼šæ›´æ–°è¿‡ç¨‹æŒ model_lockã€‚
    """
    global current_model, curr_raw, model_sig, model_loading_ms

    try:
        # 1) å°è¯•æ‹‰å– latest.txt
        latest_raw = _fetch("latest.txt")
        model_key, metrics_key = latest_raw.decode().strip().splitlines()

        # 2) æ‹‰å–æ¨¡å‹å­—èŠ‚æµå¹¶è®¡ç®—ç­¾å
        raw = _fetch(model_key)
        sig = hashlib.md5(raw).hexdigest()

        # 3) éå¼ºåˆ¶ä¸”ç­¾åæœªå˜ â†’ è·³è¿‡
        if not force and sig == model_sig:
            return

        # 4) è¯»å–éªŒè¯é›†å‡†ç¡®ç‡ï¼Œè®¡ç®—å¢ç›Š
        metrics = json.loads(_fetch(metrics_key).decode())
        new_acc  = metrics.get("acc@0.15",    0.0)
        base_acc = metrics.get("baseline_acc@0.15", 0.0)
        gain_pp  = new_acc - base_acc

        # 5) éå¼ºåˆ¶ä¸”å¢ç›Šä¸è¶³ â†’ è·³è¿‡
        if not force and gain_pp < GAIN_THR_PP:
            print(f"[infer:{pod_name}] Î”{gain_pp:+.3f} pp < {GAIN_THR_PP} â†’ skip reload")
            return

        # 6) åŠ è½½æ–°æ¨¡å‹å¹¶åŸå­æ›´æ–°
        t0 = time.perf_counter()
        mdl = torch.load(io.BytesIO(raw), map_location=device).eval()
        with model_lock:
            current_model      = mdl
            current_model._val_acc15 = new_acc
            curr_raw, model_sig     = raw, sig
        model_loading_ms = round((time.perf_counter() - t0) * 1000, 3)

        log_metric(component="infer", event="hot_reload_runtime",
                   model_loading_ms=model_loading_ms)
        print(
            f"[infer:{pod_name}] hot-reloaded âœ“  "
            f"baseline={base_acc:.2f}% â†’ new={new_acc:.2f}%  "
            f"(Î”{gain_pp:+.3f} pp)  load={model_loading_ms} ms"
        )

    except ClientError as e:
        # latest.txt ä¸å­˜åœ¨ â†’ è·³è¿‡ï¼›å…¶ä»– S3 é”™è¯¯éƒ½ log åè·³è¿‡
        code = e.response.get("Error", {}).get("Code", "")
        if code == "NoSuchKey":
            print(f"[infer:{pod_name}] reload: no latest.txt â†’ skip")
        else:
            print(f"[infer:{pod_name}] reload ClientError â†’ {e}")
        return

    except Exception as e:
        # ç½‘ç»œä¸­æ–­ã€è¿æ¥è¢«æ‹’ç­‰ä¸€åˆ‡å¼‚å¸¸éƒ½ log åè·³è¿‡
        print(f"[infer:{pod_name}] reload unexpected error â†’ {e}")
        return



# ---------- Kafka ç›‘å¬çº¿ç¨‹ï¼ˆå¸¦é‡è¯•ï¼‰ -----------------------------------------------
import time
from kafka.errors import NoBrokersAvailable

q = queue.Queue()

# å½“æ¯ä¸ªåˆ†åŒºéƒ½æ”¶åˆ°ä¸€æ¬¡ {"producer_done": true} æ—¶æ‰è®¤ä¸ºçœŸæ­£ç»“æŸ
producer_done  = threading.Event()   # å…¼å®¹æ—§é€»è¾‘ï¼šæ”¶åˆ°ä»»ä½• sentinel å°±ç½®ä½
sentinel_seen  = 0                  # å·²æ”¶åˆ°çš„ sentinel æ•°
NUM_PARTITIONS = 0                  # åˆå§‹åŒ–åç”± _listener() å®é™…å¡«å……
sentinel_lock  = threading.Lock()   # å¹¶å‘ä¿æŠ¤
producer_done = threading.Event()

# ---------- Kafka ç›‘å¬çº¿ç¨‹ï¼ˆå¸¦é‡è¯•ï¼‰ -----------------------------------------------
import time
from kafka.errors import NoBrokersAvailable

q = queue.Queue()

producer_done  = threading.Event()
sentinel_seen  = 0
NUM_PARTITIONS = 0
sentinel_lock  = threading.Lock()

def _create_consumer():
    """å°è¯•é‡è¯•è¿æ¥ Kafkaï¼Œå¤šæ¬¡å¤±è´¥åæŠ›å‡º RuntimeError"""
    for attempt in range(1, 11):
        try:
            return KafkaConsumer(
                KAFKA_TOPIC,
                bootstrap_servers=",".join(KAFKA_SERVERS),
                group_id="cg-infer",
                auto_offset_reset=AUTO_OFFSET_RESET,
                enable_auto_commit=ENABLE_AUTO_COMMIT,
                value_deserializer=lambda m: json.loads(m.decode()),
                api_version_auto_timeout_ms=10000,
            )
        except NoBrokersAvailable:
            print(f"[infer:{pod_name}] brokers unavailable ({attempt}/10), retrying in 5sâ€¦")
            time.sleep(5)
    raise RuntimeError("[infer] Kafka still unreachable after 10 retries")

def _listener():
    """
    â€¢ åˆ›å»º KafkaConsumerï¼ˆå« 10 æ¬¡é‡è¯•ï¼‰
    â€¢ ç»Ÿè®¡ topic åˆ†åŒºæ•°ï¼Œè®°å½•åˆ° NUM_PARTITIONS
    â€¢ æ¶ˆè´¹æ•°æ®ï¼š
        â€“ {"producer_done": true}  ç»Ÿè®¡åˆ†åŒºå®Œæˆ
        â€“ æ™®é€šæ ·æœ¬                 æ¨åˆ° q
    """
    global NUM_PARTITIONS, sentinel_seen

    cons = _create_consumer()
    print(f"[infer:{pod_name}] KafkaConsumer created, beginning to pollâ€¦")

    # ç­‰å¾…åˆ†åŒºåˆ†é…
    time.sleep(1)
    NUM_PARTITIONS = len(cons.partitions_for_topic(KAFKA_TOPIC) or [])
    print(f"[infer:{pod_name}] topic Â«{KAFKA_TOPIC}Â» has {NUM_PARTITIONS} partitions")

    # readiness flag
    flag_local = os.path.join(TMP_DIR, f"consumer_ready_{pod_name}.flag")
    open(flag_local, "w").close()
    save_bytes(f"{RESULT_DIR}/consumer_ready_{pod_name}.flag", b"", "text/plain")

    for msg in cons:
        v = msg.value
        if v.get("producer_done"):                  # ç”Ÿäº§è€…ç»“æŸæ ‡è®°
            with sentinel_lock:
                sentinel_seen += 1
                print(f"[infer:{pod_name}] got sentinel "
                      f"{sentinel_seen}/{NUM_PARTITIONS}")
            producer_done.set()
            continue

        v["_recv_ts"] = datetime.utcnow().isoformat() + "Z"
        q.put(v)


# å¯åŠ¨ç›‘å¬çº¿ç¨‹ï¼ˆdaemon æ¨¡å¼ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹ï¼‰
threading.Thread(target=_listener, daemon=True).start()

# -----------------------------------------------------------------------------
# å‘¨æœŸæ€§åå°çƒ­é‡è½½ï¼ˆæ¯ RELOAD_INTERVAL_S ç§’è°ƒç”¨ä¸€æ¬¡ _reload_modelï¼‰
# -----------------------------------------------------------------------------
RELOAD_INTERVAL_S = int(os.getenv("RELOAD_INTERVAL_S", "30"))

def _reload_daemon():
    while True:
        time.sleep(RELOAD_INTERVAL_S)
        try:
            _reload_model()
        except Exception as e:
            # ç†è®ºä¸Š _reload_model å·²ç»åæ‰æ‰€æœ‰å¼‚å¸¸ï¼Œè¿™é‡Œåšå…œåº•
            print(f"[infer:{pod_name}] reload daemon error â†’ {e}")

# å¯åŠ¨å®ˆæŠ¤çº¿ç¨‹
threading.Thread(target=_reload_daemon, daemon=True).start()

# åºŸå¼ƒåŸ per-batch çƒ­é‡è½½ï¼Œå°† hot_reload æ”¹ä¸ºç©ºå®ç°
def hot_reload():
    # æ¯ä¸ª batch ä¸å†è§¦å‘çƒ­é‡è½½ï¼Œæ”¹ç”¨ä¸Šé¢çš„å®ˆæŠ¤çº¿ç¨‹
    pass
# -----------------------------------------------------------------------------

def _take_batch():
    buf = []
    try: buf.append(q.get(timeout=CONSUME_IDLE_S))
    except queue.Empty: return buf
    while len(buf) < BATCH_SIZE:
        try: buf.append(q.get_nowait())
        except queue.Empty: break
    return buf


trigger_prod = KafkaProducer(
    bootstrap_servers=",".join(KAFKA_SERVERS),
    value_serializer=lambda m: json.dumps(m).encode(),
)

# ---------- Forecasting (demo) -------------------------------------------
forecast_hist = deque(maxlen=300)
def _forecast_loop():
    while True:
        time.sleep(30)
        if not forecast_hist: continue
        with Timer("Forecasting_Engine", "infer"): _ = float(np.mean(forecast_hist))
        log_metric(component="infer", event="forecasting_runtime")
threading.Thread(target=_forecast_loop, daemon=True).start()


def _align_to_dim(X_scaled: np.ndarray, in_dim: int) -> np.ndarray:
    """
    æŠŠ 60-ç»´ Scaler ç‰¹å¾è£å‰ª/è¡¥é›¶åˆ°ç›®æ ‡ in_dimã€‚
    """
    if in_dim == X_scaled.shape[1]:
        return X_scaled
    elif in_dim < X_scaled.shape[1]:
        return X_scaled[:, :in_dim]          # æˆªæ–­
    else:
        pad = np.zeros((X_scaled.shape[0], in_dim - X_scaled.shape[1]),
                       dtype=np.float32)
        return np.concatenate([X_scaled, pad], axis=1)


# ---------------------------  ä¸»å¾ªç¯  ------------------------------------
first_batch     = True
container_start = time.perf_counter()
IDLE_TIMEOUT_S  = 30      # è¶…è¿‡ 30 ç§’æ— æ–°æ¶ˆæ¯å°±é€€å‡º
last_data_time  = time.time()
msg_total       = 0
correct_count   = 0
total_count     = 0

print(f"[infer:{pod_name}] consumer startedâ€¦")

while True:
    batch = _take_batch()
    now   = time.time()

    # å¦‚æœè¿™æ®µæ—¶é—´æ—¢æ²¡æ•°æ®åˆè¶…æ—¶ï¼Œå°±ä¼˜é›…é€€å‡º
    if not batch and (now - last_data_time) > IDLE_TIMEOUT_S:
        print(f"[infer:{pod_name}] idle >{IDLE_TIMEOUT_S}s, exiting")
        try:
            _reload_model(force=True)
        except Exception as e:
            print(f"[infer:{pod_name}] final reload error â†’ {e}")
        print(f"[infer:{pod_name}] graceful shutdown â€“ processed {msg_total} samples")
        break

    if not batch:
        with sentinel_lock:
            all_done = (sentinel_seen >= NUM_PARTITIONS) if NUM_PARTITIONS else False
        if all_done and q.empty():
            try:
                _reload_model(force=True)
            except Exception as e:
                print(f"[infer:{pod_name}] final reload error â†’ {e}")
            print(f"[infer:{pod_name}] graceful shutdown â€“ processed {msg_total} samples")
            break
        time.sleep(0.3)
        continue

    # æ”¶åˆ°æ•°æ®ï¼Œæ›´æ–°æ—¶é—´æˆ³
    last_data_time = now
    if first_batch:
        cold_ms = (time.perf_counter() - container_start) * 1000
        log_metric(component="infer", event="cold_start",
                   cold_start_ms=round(cold_ms, 3))
        first_batch = False

    # 1) Extraction --------------------------------------------------------
    with Timer("Extraction", "infer"):
        rows_batch = list(batch)

    # 2) Preprocessing -----------------------------------------------------
    with Timer("Preprocessing", "infer"):
        # â‘  æå– top-10 åŸå§‹ç‰¹å¾
        X_raw = np.array(
            [[r["features"].get(c, 0.0) for c in SELECTED_FEATS]
             for r in rows_batch],
            dtype=np.float32
        )
        # â‘¡ æ ‡å‡†åŒ–
        X_scaled = scaler.transform(X_raw)

    # â”€â”€ çº¿ç¨‹å®‰å…¨åœ°æ‹¿ä¸€ä»½å½“å‰ adaptive æ¨¡å‹å¼•ç”¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with model_lock:
        model_ref = current_model          # åªåœ¨ä¸´ç•ŒåŒºåšå¼•ç”¨æ‹·è´

    # === å…³é”®ï¼šè®©ä¸¤è·¯è¾“å…¥ä¸å„è‡ªæ¨¡å‹é¦–å±‚ç»´åº¦åŒ¹é… ============================
    X_base = _align_to_dim(X_scaled, baseline_in_dim)   # åŸºçº¿æ¨¡å‹å›ºå®š 10 ç»´
    X_adpt = _align_to_dim(X_scaled, model_ref.net[0].in_features)
        
        
    # 3) Inference ---------------------------------------------------------
    cpu0, t0 = proc.cpu_times(), time.perf_counter()
    with Timer("Inference_Engine", "infer"):
        with torch.no_grad():
            # Baseline é¢„æµ‹
            preds_base = baseline_model(
                torch.from_numpy(X_base).to(device)
            ).cpu().numpy().ravel()
            # Adaptive é¢„æµ‹ï¼ˆä½¿ç”¨åˆšæ‰çº¿ç¨‹å®‰å…¨å–åˆ°çš„ model_refï¼‰
            preds_adpt = model_ref(
                torch.from_numpy(X_adpt).to(device)
            ).cpu().numpy().ravel()
    cpu1, t1 = proc.cpu_times(), time.perf_counter()

    # 4) Accuracy@0.2 ç»Ÿè®¡ + ç†”æ–­é€»è¾‘ -------------------------------------
    labels = np.array([r["label"] for r in rows_batch], np.float32)
    errs   = np.abs(preds_adpt - labels) / np.maximum(labels, 1e-8)
    batch_correct = int((errs <= 0.2).sum())
    batch_total   = len(labels)

    correct_count += batch_correct
    total_count   += batch_total
    cum_acc = correct_count / total_count

    print(f"[infer:{pod_name}] accuracy@0.2 â†’ "
          f"batch {batch_correct}/{batch_total}, "
          f"cumulative {correct_count}/{total_count} = {cum_acc:.3f}")

    log_metric(
        component="infer",
        event="cumulative_accuracy",
        threshold=0.2,
        batch_correct=batch_correct,
        batch_total=batch_total,
        cumulative_correct=correct_count,
        cumulative_total=total_count,
        cumulative_accuracy=round(cum_acc, 3)
    )

    # â€”â€” è¿ç»­çª—å£ç†”æ–­ ------------------------------------------------------
    if not hasattr(_align_to_dim, "acc_deque"):
        from collections import deque
        _align_to_dim.acc_deque = deque(maxlen=ACC_WINDOW)  # 300
        _align_to_dim.low_seq   = 0                         # è¿ç»­ä½å‡†ç¡®çª—å£è®¡æ•°

    # æŠŠå½“å‰ batch çš„é€æ ·æœ¬å‘½ä¸­æƒ…å†µåŠ å…¥æ»‘åŠ¨çª—å£
    _align_to_dim.acc_deque.extend(
        [1] * batch_correct + [0] * (batch_total - batch_correct)
    )

    if len(_align_to_dim.acc_deque) == ACC_WINDOW:
        win_acc = sum(_align_to_dim.acc_deque) / ACC_WINDOW
        if win_acc < ACC_THRESHOLD:                # ä½äºé˜ˆå€¼
            _align_to_dim.low_seq += 1
        else:
            _align_to_dim.low_seq = 0              # è¾¾æ ‡å°±æ¸…é›¶

        if _align_to_dim.low_seq >= LOW_MAX:       # è¿ç»­ LOW_MAX æ¬¡éƒ½ä½
            try:
                trigger_prod.send(
                    RETRAIN_TOPIC,
                    {
                        "force_retrain": "K",       # æœ€é«˜ç­‰çº§
                        "win_acc": round(win_acc, 3)
                    }
                )
                print(f"[infer:{pod_name}] force_retrain K sent (win_acc={win_acc:.3f})")
            except Exception as e:
                print(f"[infer:{pod_name}] force_retrain send error â†’ {e}")
            finally:
                _align_to_dim.low_seq = 0          # å‘é€åç«‹å³å¤ä½

    # ---------- DEBUGï¼šè¯¯å·®åˆ†å¸ƒï¼ˆåŸºçº¿ vs. è‡ªé€‚åº”ï¼‰ ----------
    # â‘  ç›¸å¯¹è¯¯å·®
    err_base = np.abs(preds_base - labels) / np.maximum(labels, 1e-8)
    err_adpt = np.abs(preds_adpt - labels) / np.maximum(labels, 1e-8)

    # â‘¡ å…³é”®åˆ†ä½æ•°
    pct = [50, 80, 90, 95, 99]
    base_q = np.percentile(err_base, pct).round(3)
    adpt_q = np.percentile(err_adpt, pct).round(3)

    print(
        f"[infer:{pod_name}]  Î”(relative) | "
        f"BASE p50/p80/p90/p95/p99 = {base_q.tolist()} | "
        f"ADPT = {adpt_q.tolist()}"
    )

    log_metric(
        component="infer",
        event="err_dist",
        base_p50=float(base_q[0]), base_p80=float(base_q[1]),
        base_p90=float(base_q[2]), base_p95=float(base_q[3]),
        base_p99=float(base_q[4]),
        adpt_p50=float(adpt_q[0]), adpt_p80=float(adpt_q[1]),
        adpt_p90=float(adpt_q[2]), adpt_p95=float(adpt_q[3]),
        adpt_p99=float(adpt_q[4]),
    )
    
    # ---------- æ”¶é›†æŒ‡æ ‡ & æ—¥å¿— ------------------------------------------
    ts_hist.extend([r["send_ts"] for r in rows_batch])
    # 5) åŸæœ‰æŒ‡æ ‡ & æ—¥å¿—
    pred_orig_hist.extend(preds_base.tolist())
    pred_adj_hist .extend(preds_adpt.tolist())
    true_hist     .extend(labels.tolist())

    # è®¡ç®— RTTã€CPU%ã€TPS ç­‰
    rtts = []
    for r in rows_batch:
        if "send_ts" not in r:
            continue
        try:
            st = datetime.fromisoformat(r["send_ts"].rstrip("Z"))
            rt = (datetime.fromisoformat(r["_recv_ts"].rstrip("Z")) - st
                  ).total_seconds() * 1000
            rtts.append(rt)
        except Exception:
            pass
    avg_rtt = round(np.mean(rtts), 3) if rtts else 0.0
    wall_ms = (t1 - t0) * 1000
    cpu_used = ((cpu1.user + cpu1.system) - (cpu0.user + cpu0.system)) * 1000
    cpu_pct = round(cpu_used / wall_ms, 2) if wall_ms else 0.0
    tp_s    = round(len(rows_batch) / (wall_ms or 1e-3), 3)

    log_metric(
        component="infer", event="batch_metrics",
        batch_size=len(rows_batch),
        latency_ms=round(wall_ms, 3),
        throughput_s=tp_s,
        cpu_pct=cpu_pct,
        gpu_mem_pct=0.0,
        model_loading_ms=model_loading_ms,
        container_latency_ms=round(wall_ms, 3),
        rtt_ms=avg_rtt
    )
    msg_total += len(rows_batch)
    print(f"[infer:{pod_name}] batch_metrics: "
          f"msg_total={msg_total}, latency={wall_ms:.3f}ms, "
          f"avg_rtt={avg_rtt}ms, cpu_pct={cpu_pct}/s")
    try:
        trigger_prod.send(RETRAIN_TOPIC, {"processed": batch_total})
    except Exception:
        pass
    # 6) Forecast & Hot-reload ä¿æŒä¸å˜
    forecast_hist.extend(preds_adpt)
    hot_reload()

# 7) æ”¶å°¾ï¼šä¿å­˜æ•°ç»„ & ä¸Šä¼ 
print(f"[infer:{pod_name}] TOTAL processed {msg_total} samples, exit")


# ---------- æ”¶å°¾ï¼šä¿å­˜å®Œæ•´ trace å¹¶ä¸Šä¼  ----------------------------------
print(f"[infer:{pod_name}] TOTAL processed {msg_total} samples, exit")

arr_adj   = np.asarray(pred_adj_hist , np.float32)
arr_orig  = np.asarray(pred_orig_hist, np.float32)
arr_true  = np.asarray(true_hist     , np.float32)
arr_ts    = np.asarray([
    datetime.fromisoformat(t.rstrip("Z")).timestamp()
    for t in ts_hist
], np.float64)

npz_local = os.path.join(local_out, "inference_trace.npz")
np.savez(npz_local,     # ä¿å­˜ä¸º npz æ ¼å¼ æ•°æ®
         ts=arr_ts,     # æ¯æ¡æ ·æœ¬çš„å‘é€æ—¶é—´æˆ³
         pred_adj=arr_adj,  # çƒ­æ›´æ–°æ¨¡å‹çš„é¢„æµ‹å€¼åºåˆ—
         pred_orig=arr_orig,  # åŸºçº¿æ¨¡å‹çš„é¢„æµ‹å€¼åºåˆ—
         true=arr_true)  # çœŸå®æ ‡ç­¾åºåˆ—, ç”¨æ¥ä¹‹åç®—ååé‡

with open(npz_local, "rb") as f:
    save_bytes(f"{RESULT_DIR}/{pod_name}_inference_trace.npz",
               f.read(), "application/octet-stream")
print(f"[infer:{pod_name}] trace npz saved â€“ total {len(arr_true)} samples")

sync_all_metrics_to_minio()
print(f"[infer:{pod_name}] all metrics synced, exiting.")


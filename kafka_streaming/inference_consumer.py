#!/usr/bin/env python3
"""
kafka_streaming/inference_consumer.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
baseline  : offline è®­ç»ƒå¾—åˆ°çš„ MLP Scalerâ†’PCAâ†’N ç»´è¾“å…¥
adaptive  : çƒ­æ›´æ–°æ¨¡å‹ Scaler åç›´æ¥åƒ full_dim ç‰¹å¾
ä¸¤è·¯é¢„æµ‹ + çœŸå®å€¼ å…¨é‡è®°å½•ï¼Œä¾› plot_final.py ç›´æ¥æ‹¼ Phase-1/2/3
æ‰€æœ‰åŸæœ‰æŒ‡æ ‡åŸ‹ç‚¹ ,æ—¥å¿—å®Œå…¨ä¿ç•™
"""

import os, io, json, time, queue, threading, hashlib
from datetime import datetime
from collections import deque
from typing import List

import threading
import numpy as np
import joblib
import psutil
import torch
import torch.nn as nn
from kafka import KafkaConsumer

from shared.utils import _fetch, _bytes_to_model
from shared.metric_logger import log_metric, sync_all_metrics_to_minio
from shared.profiler      import Timer
from shared.minio_helper  import s3, save_bytes, save_np, BUCKET
from shared.config        import (
    KAFKA_TOPIC, KAFKA_SERVERS, BATCH_SIZE, CONSUME_IDLE_S,
    MODEL_DIR, RESULT_DIR, AUTO_OFFSET_RESET, ENABLE_AUTO_COMMIT
)
from shared.features      import FEATURE_COLS
from ml.model             import DynamicMLP, build_model

from botocore.exceptions import ClientError     

# ---------- å¸¸é‡ & æœ¬åœ°è·¯å¾„ ---------------------------------------------
model_lock = threading.Lock()
MODEL_IMPROVE_EPS = float(os.getenv("MODEL_IMPROVE_EPS", "1.0"))  # %

TMP_DIR  = "/tmp/infer"                     # â† æ”¹ç”¨ /tmp
os.makedirs(TMP_DIR, exist_ok=True)

pod_name = os.getenv("HOSTNAME", "infer")   # k8s å®¹å™¨å

local_out = os.path.join(TMP_DIR, pod_name) # æ¯ä¸ª consumer ç‹¬ç«‹ç›®å½•
os.makedirs(local_out, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
proc   = psutil.Process()

pred_orig_hist: List[float] = []
pred_adj_hist : List[float] = []
true_hist     : List[float] = []
ts_hist       : List[str]   = []

# ---------- é€‰ç‰¹å¾ & scaler åŠ è½½ ------------------------------------------
import json, io, joblib

# 1) æ‹‰å–å¹¶åŠ è½½ä¸ output_rate æœ€ç›¸å…³çš„ top10 ç‰¹å¾åˆ—è¡¨
raw_feats = _fetch("selected_feats.json")
SELECTED_FEATS = json.loads(raw_feats)
print(f"[infer:{pod_name}] using selected feats: {SELECTED_FEATS}")

# 2) æ‹‰å–å¹¶åŠ è½½å¯¹åº”çš„ StandardScaler
with open(f"{local_out}/scaler.pkl", "wb") as f:
    f.write(_fetch("scaler.pkl"))
scaler = joblib.load(f"{local_out}/scaler.pkl")

# ä¸å†ä½¿ç”¨ PCA
pca = None
use_pca = False

# ---------- å®Œæ•´æ¨¡å‹åŠ è½½å·¥å…·å‡½æ•° ------------------------------------------
def _load_full_model(key: str) -> tuple[nn.Module, bytes]:
    """
    ä» MinIO æ‹‰å– key å¯¹åº”æ–‡ä»¶ï¼Œç¡®ä¿å®ƒæ˜¯å®Œæ•´ torch.save(model) å¯¼å‡ºçš„ nn.Moduleã€‚
    å¦‚æœæ”¶åˆ° OrderedDictï¼ˆstate_dictï¼‰ï¼Œ_bytes_to_model ä¼šæŠ› TypeErrorã€‚
    è¿”å› (model, raw_bytes)ã€‚
    """
    raw = _fetch(key)
    model = _bytes_to_model(raw).to(device)  # _bytes_to_model å·²ç» .eval()
    return model, raw

# ---------- baseline & adaptive æ¨¡å‹è£…è½½ ----------------------------------
baseline_model, base_raw = _load_full_model("baseline_model.pt")
baseline_in_dim          = baseline_model.net[0].in_features

current_model, curr_raw  = _load_full_model("model.pt")
current_model._val_acc15 = 0.0          # åˆå§‹åŸºçº¿

print(f"[infer:{pod_name}] baseline in_features = {baseline_in_dim}")
print(f"[infer:{pod_name}] adaptive model       = {current_model}")

model_sig        = hashlib.md5(curr_raw).hexdigest()
model_loading_ms = 0.0


# ---------- çƒ­é‡è½½ --------------------------------------------------------
GAIN_THR_PP = float(os.getenv("GAIN_THRESHOLD_PP", "0.001"))  # â‰¥0.x ä¸ªç™¾åˆ†ç‚¹å°±æ¢

def _reload_model(force: bool = False):
    """
    çƒ­åŠ è½½ï¼šä»…å½“æ–°æ¨¡å‹ç›¸å¯¹ baseline ç²¾åº¦æå‡ >= GAIN_THR_PP
    æˆ–è€… force=True æ—¶æ‰æ›¿æ¢ current_modelã€‚
    ğŸ”’ çº¿ç¨‹å®‰å…¨ï¼šæ›´æ–°è¿‡ç¨‹æŒ model_lockã€‚
    """
    global current_model, curr_raw, model_sig, model_loading_ms

    try:
        latest_raw = _fetch("latest.txt")
    except ClientError as e:
        if e.response["Error"]["Code"] == "NoSuchKey":
            return
        raise

    model_key, metrics_key = latest_raw.decode().strip().splitlines()
    raw = _fetch(model_key)
    sig = hashlib.md5(raw).hexdigest()

    # 1) å¦‚æœç‰ˆæœ¬æœªå˜ï¼Œç›´æ¥è·³è¿‡
    if not force and sig == model_sig:
        return

    # 2) è¯»å–æ–°æ—§æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šçš„ acc@0.15
    metrics = json.loads(_fetch(metrics_key).decode())
    new_acc  = metrics.get("acc@0.15", 0.0)
    base_acc = metrics.get("baseline_acc@0.15", 0.0)
    gain_pp  = new_acc - base_acc

    # 3) åªæœ‰å½“æå‡ >= GAIN_THR_PP æ—¶æ‰æ›¿æ¢
    if not force and gain_pp < GAIN_THR_PP:
        print(
            f"[infer:{pod_name}] Î”{gain_pp:+.3f} pp < {GAIN_THR_PP} â†’ skip reload"
        )
        return

    # 4) åŠ è½½æ–°æ¨¡å‹å¹¶æ›¿æ¢
    t0 = time.perf_counter()
    mdl = torch.load(io.BytesIO(raw), map_location=device).eval()
    with model_lock:
        current_model      = mdl
        current_model._val_acc15 = new_acc
        curr_raw, model_sig = raw, sig
    model_loading_ms = round((time.perf_counter() - t0) * 1000, 3)

    log_metric(component="infer", event="hot_reload_runtime",
               model_loading_ms=model_loading_ms)
    print(
        f"[infer:{pod_name}] hot-reloaded âœ“  "
        f"baseline={base_acc:.2f}% â†’ new={new_acc:.2f}%  "
        f"(Î”{gain_pp:+.3f} pp)  load={model_loading_ms} ms"
    )

def hot_reload():
    """å¼‚æ­¥è§¦å‘çƒ­é‡è½½ï¼Œä¸é˜»å¡ä¸»æ¨ç†å¾ªç¯ã€‚"""
    threading.Thread(target=_reload_model, daemon=True).start()




# ---------- Kafka ç›‘å¬çº¿ç¨‹ -----------------------------------------------
q = queue.Queue()
producer_done = threading.Event()

def _listener():
    cons = KafkaConsumer(
        KAFKA_TOPIC,
        bootstrap_servers=",".join(KAFKA_SERVERS),
        group_id="cg-infer",
        auto_offset_reset=AUTO_OFFSET_RESET,
        enable_auto_commit=ENABLE_AUTO_COMMIT,
        value_deserializer=lambda m: json.loads(m.decode()),
    )

    # readiness flagï¼š/tmp + MinIO
    flag_local = os.path.join(TMP_DIR, f"consumer_ready_{pod_name}.flag")
    open(flag_local, "w").close()
    print(f"[infer:{pod_name}] readiness flag â†’", flag_local)

    save_bytes(f"{RESULT_DIR}/consumer_ready_{pod_name}.flag",
               b"", "text/plain")

    for msg in cons:    # ç›‘å¬ Kafka æ¶ˆæ¯
        v = msg.value
        if v.get("producer_done"):
            producer_done.set()
            continue
        v["_recv_ts"] = datetime.utcnow().isoformat() + "Z"   # å†æ¬¡æ³¨å…¥ UTC æ—¶é—´æˆ³ï¼Œä¸ºæ¥æ”¶æ—¶é—´æˆ³ï¼Œä¹Ÿä¿ç•™äº†åŸå§‹çš„ v["send_ts"]
        q.put(v)

threading.Thread(target=_listener, daemon=True).start()

def _take_batch():
    buf = []
    try: buf.append(q.get(timeout=CONSUME_IDLE_S))
    except queue.Empty: return buf
    while len(buf) < BATCH_SIZE:
        try: buf.append(q.get_nowait())
        except queue.Empty: break
    return buf

# ---------- Forecasting (demo) -------------------------------------------
forecast_hist = deque(maxlen=300)
def _forecast_loop():
    while True:
        time.sleep(30)
        if not forecast_hist: continue
        with Timer("Forecasting_Engine", "infer"): _ = float(np.mean(forecast_hist))
        log_metric(component="infer", event="forecasting_runtime")
threading.Thread(target=_forecast_loop, daemon=True).start()

def _align_adaptive_input(X_scaled: np.ndarray, model: nn.Module) -> np.ndarray:
    in_dim = model.net[0].in_features
    if in_dim == X_scaled.shape[1]:
        X_aligned = X_scaled
    elif in_dim < X_scaled.shape[1]:
        X_aligned = X_scaled[:, :in_dim]          # æˆªæ–­
    else:
        pad = np.zeros((X_scaled.shape[0], in_dim - X_scaled.shape[1]),
                       dtype=np.float32)
        X_aligned = np.concatenate([X_scaled, pad], axis=1)

    # === DEBUG 2ï¼šè¾“å…¥æ˜¯ä¸æ˜¯å…¨ 0ï¼Ÿ =============================
    if not np.any(X_aligned):
        print(f"[infer:{pod_name}] DEBUG â‘¡  aligned input **ALL ZERO** !")
    return X_aligned

def _align_to_dim(X_scaled: np.ndarray, in_dim: int) -> np.ndarray:
    """
    æŠŠ 60-ç»´ Scaler ç‰¹å¾è£å‰ª/è¡¥é›¶åˆ°ç›®æ ‡ in_dimã€‚
    """
    if in_dim == X_scaled.shape[1]:
        return X_scaled
    elif in_dim < X_scaled.shape[1]:
        return X_scaled[:, :in_dim]          # æˆªæ–­
    else:
        pad = np.zeros((X_scaled.shape[0], in_dim - X_scaled.shape[1]),
                       dtype=np.float32)
        return np.concatenate([X_scaled, pad], axis=1)


def _make_input(model: nn.Module, X_scaled: np.ndarray) -> np.ndarray:
    """
    æ ¹æ® **æ¨¡å‹é¦–å±‚ in_features** è‡ªåŠ¨å†³å®šï¼š
        â€¢ ç”¨ PCA ç‰¹å¾       â€”â€” è‹¥ in_dim == pca.n_components_
        â€¢ ç”¨ Scaler ç‰¹å¾å¯¹é½ â€”â€” å¦åˆ™
    è¿™æ ·æ— è®º adaptive æ¨¡å‹æ˜¯ 6-ç»´ï¼ˆPCAï¼‰è¿˜æ˜¯ 60-ç»´ï¼ˆå…¨ç‰¹å¾ï¼‰éƒ½èƒ½å–‚å¯¹è¾“å…¥ã€‚
    """
    in_dim = model.net[0].in_features
    if use_pca and in_dim == pca.n_components_:
        return pca.transform(X_scaled).astype(np.float32)
    return _align_to_dim(X_scaled, in_dim)

# ---------------------------  ä¸»å¾ªç¯  ------------------------------------
first_batch     = True
container_start = time.perf_counter()
IDLE_TIMEOUT_S  = 180
last_data_time  = time.time()
msg_total       = 0
correct_count   = 0
total_count     = 0

print(f"[infer:{pod_name}] consumer startedâ€¦")

while True:
    batch = _take_batch()
    now   = time.time()

    if not batch:
        hot_reload()
        # è‹¥å·²æ”¶åˆ° producer_done æˆ–é•¿æ—¶é—´æ— æ•°æ®åˆ™ç»“æŸ
        if producer_done.is_set() or (now - last_data_time) > IDLE_TIMEOUT_S:
            _reload_model(force=True)
            break
        time.sleep(0.3)
        continue

    last_data_time = now
    if first_batch:
        cold_ms = (time.perf_counter() - container_start) * 1000
        log_metric(component="infer", event="cold_start",
                   cold_start_ms=round(cold_ms, 3))
        first_batch = False

    # 1) Extraction --------------------------------------------------------
    with Timer("Extraction", "infer"):
        rows_batch = list(batch)

    # 2) Preprocessing -----------------------------------------------------
    with Timer("Preprocessing", "infer"):
        # â‘  æå– top-10 åŸå§‹ç‰¹å¾
        X_raw = np.array(
            [[r["features"].get(c, 0.0) for c in SELECTED_FEATS]
             for r in rows_batch],
            dtype=np.float32
        )
        # â‘¡ æ ‡å‡†åŒ–
        X_scaled = scaler.transform(X_raw)

    # â”€â”€ çº¿ç¨‹å®‰å…¨åœ°æ‹¿ä¸€ä»½å½“å‰ adaptive æ¨¡å‹å¼•ç”¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with model_lock:
        model_ref = current_model          # åªåœ¨ä¸´ç•ŒåŒºåšå¼•ç”¨æ‹·è´

    # === å…³é”®ï¼šè®©ä¸¤è·¯è¾“å…¥ä¸å„è‡ªæ¨¡å‹é¦–å±‚ç»´åº¦åŒ¹é… ============================
    X_base = _align_to_dim(X_scaled, baseline_in_dim)   # åŸºçº¿æ¨¡å‹å›ºå®š 10 ç»´
    X_adpt = _make_input(model_ref, X_scaled)           # adaptive å¯èƒ½æ˜¯ 10 æˆ– 60 ç»´
        
        
    # 3) Inference ---------------------------------------------------------
    cpu0, t0 = proc.cpu_times(), time.perf_counter()
    with Timer("Inference_Engine", "infer"):
        with torch.no_grad():
            # Baseline é¢„æµ‹
            preds_base = baseline_model(
                torch.from_numpy(X_base).to(device)
            ).cpu().numpy().ravel()
            # Adaptive é¢„æµ‹ï¼ˆä½¿ç”¨åˆšæ‰çº¿ç¨‹å®‰å…¨å–åˆ°çš„ model_refï¼‰
            preds_adpt = model_ref(
                torch.from_numpy(X_adpt).to(device)
            ).cpu().numpy().ravel()
    cpu1, t1 = proc.cpu_times(), time.perf_counter()

    # 4) Accuracy@0.2 ç»Ÿè®¡
    labels = np.array([r["label"] for r in rows_batch], np.float32)
    errs   = np.abs(preds_adpt - labels) / np.maximum(labels, 1e-8)
    batch_correct = int((errs <= 0.2).sum())
    batch_total   = len(labels)
    correct_count += batch_correct
    total_count   += batch_total
    cum_acc = correct_count / total_count
    print(f"[infer:{pod_name}] accuracy@0.2 â†’ batch {batch_correct}/{batch_total}, "
          f"cumulative {correct_count}/{total_count} = {cum_acc:.3f}")
    log_metric(
        component="infer",
        event="cumulative_accuracy",
        threshold=0.2,
        batch_correct=batch_correct,
        batch_total=batch_total,
        cumulative_correct=correct_count,
        cumulative_total=total_count,
        cumulative_accuracy=round(cum_acc, 3)
    )
    
    # ---------- DEBUGï¼šè¯¯å·®åˆ†å¸ƒï¼ˆåŸºçº¿ vs. è‡ªé€‚åº”ï¼‰ ----------
    # â‘  ç›¸å¯¹è¯¯å·®
    err_base = np.abs(preds_base - labels) / np.maximum(labels, 1e-8)
    err_adpt = np.abs(preds_adpt - labels) / np.maximum(labels, 1e-8)

    # â‘¡ å…³é”®åˆ†ä½æ•°
    pct = [50, 80, 90, 95, 99]
    base_q = np.percentile(err_base, pct).round(3)
    adpt_q = np.percentile(err_adpt, pct).round(3)

    print(
        f"[infer:{pod_name}]  Î”(relative) | "
        f"BASE p50/p80/p90/p95/p99 = {base_q.tolist()} | "
        f"ADPT = {adpt_q.tolist()}"
    )

    log_metric(
        component="infer",
        event="err_dist",
        base_p50=float(base_q[0]), base_p80=float(base_q[1]),
        base_p90=float(base_q[2]), base_p95=float(base_q[3]),
        base_p99=float(base_q[4]),
        adpt_p50=float(adpt_q[0]), adpt_p80=float(adpt_q[1]),
        adpt_p90=float(adpt_q[2]), adpt_p95=float(adpt_q[3]),
        adpt_p99=float(adpt_q[4]),
    )
    
    # ---------- æ”¶é›†æŒ‡æ ‡ & æ—¥å¿— ------------------------------------------
    ts_hist.extend([r["send_ts"] for r in rows_batch])
    # 5) åŸæœ‰æŒ‡æ ‡ & æ—¥å¿—
    pred_orig_hist.extend(preds_base.tolist())
    pred_adj_hist .extend(preds_adpt.tolist())
    true_hist     .extend(labels.tolist())

    # è®¡ç®— RTTã€CPU%ã€TPS ç­‰
    rtts = []
    for r in rows_batch:
        if "send_ts" not in r:
            continue
        try:
            st = datetime.fromisoformat(r["send_ts"].rstrip("Z"))
            rt = (datetime.fromisoformat(r["_recv_ts"].rstrip("Z")) - st
                  ).total_seconds() * 1000
            rtts.append(rt)
        except Exception:
            pass
    avg_rtt = round(np.mean(rtts), 3) if rtts else 0.0
    wall_ms = (t1 - t0) * 1000
    cpu_used = ((cpu1.user + cpu1.system) - (cpu0.user + cpu0.system)) * 1000
    cpu_pct = round(cpu_used / wall_ms, 2) if wall_ms else 0.0
    tp_s    = round(len(rows_batch) / (wall_ms or 1e-3), 3)

    log_metric(
        component="infer", event="batch_metrics",
        batch_size=len(rows_batch),
        latency_ms=round(wall_ms, 3),
        throughput_s=tp_s,
        cpu_pct=cpu_pct,
        gpu_mem_pct=0.0,
        model_loading_ms=model_loading_ms,
        container_latency_ms=round(wall_ms, 3),
        rtt_ms=avg_rtt
    )
    msg_total += len(rows_batch)
    print(f"[infer:{pod_name}] batch_metrics: "
          f"msg_total={msg_total}, latency={wall_ms:.3f}ms, "
          f"avg_rtt={avg_rtt}ms, cpu_pct={cpu_pct}/s")

    # 6) Forecast & Hot-reload ä¿æŒä¸å˜
    forecast_hist.extend(preds_adpt)
    hot_reload()

# 7) æ”¶å°¾ï¼šä¿å­˜æ•°ç»„ & ä¸Šä¼ 
print(f"[infer:{pod_name}] TOTAL processed {msg_total} samples, exit")


# ---------- æ”¶å°¾ï¼šä¿å­˜å®Œæ•´ trace å¹¶ä¸Šä¼  ----------------------------------
print(f"[infer:{pod_name}] TOTAL processed {msg_total} samples, exit")

arr_adj   = np.asarray(pred_adj_hist , np.float32)
arr_orig  = np.asarray(pred_orig_hist, np.float32)
arr_true  = np.asarray(true_hist     , np.float32)
arr_ts    = np.asarray([
    datetime.fromisoformat(t.rstrip("Z")).timestamp()
    for t in ts_hist
], np.float64)

npz_local = os.path.join(local_out, "inference_trace.npz")
np.savez(npz_local,     # ä¿å­˜ä¸º npz æ ¼å¼ æ•°æ®
         ts=arr_ts,     # æ¯æ¡æ ·æœ¬çš„å‘é€æ—¶é—´æˆ³
         pred_adj=arr_adj,  # çƒ­æ›´æ–°æ¨¡å‹çš„é¢„æµ‹å€¼åºåˆ—
         pred_orig=arr_orig,  # åŸºçº¿æ¨¡å‹çš„é¢„æµ‹å€¼åºåˆ—
         true=arr_true)  # çœŸå®æ ‡ç­¾åºåˆ—, ç”¨æ¥ä¹‹åç®—ååé‡

with open(npz_local, "rb") as f:
    save_bytes(f"{RESULT_DIR}/{pod_name}_inference_trace.npz",
               f.read(), "application/octet-stream")
print(f"[infer:{pod_name}] trace npz saved â€“ total {len(arr_true)} samples")

sync_all_metrics_to_minio()
print(f"[infer:{pod_name}] all metrics synced, exiting.")

